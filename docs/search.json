[
  {
    "objectID": "vektor_embedding.html",
    "href": "vektor_embedding.html",
    "title": "Vektor-Embedding",
    "section": "",
    "text": "Erstmal ein kleiner Gag um darauf hinzuweisen, wie gut Sortieralgorithmen bereits funktionieren (nicht KI).\nEin beliebtes Spiel in meiner Schulzeit war der Akinator\n\\(\\rightarrow\\) Klicke bitte auf Spielen in der Mitte \\(\\leftarrow\\)",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Vektor-Embedding"
    ]
  },
  {
    "objectID": "vektor_embedding.html#wie-funktioniert-das",
    "href": "vektor_embedding.html#wie-funktioniert-das",
    "title": "Vektor-Embedding",
    "section": "Wie funktioniert das?",
    "text": "Wie funktioniert das?\nJeder Figur wird ein Vektor zugeordnet, die Einträge stehen für bestimmte Eigenschaften.\nDas heißt ein Vektor könnte z.B. so aussehen. \\[(Größe, Haarfarbe, real/fiktiv, Alter, ...)\\]\nMan sieht schnell die Einträge können unterschiedliche Formen/Definitionen haben, aber das Prinzip ist schnell verstanden.\n\n\n\n\n\n\nNoneUmsetzung mit Python\n\n\n\n\n\nIm folgenden ein Beispiel Code, mit dem du ein bisschen die Grundidee von Vektor-Embedding testen kannst.\nDu kannst den Code ausführen, nachdem du deine Werte eingegeben hast, mit dem Run-Symbol oben rechts. Du kannst auch gerne weitere Personen zum variablen-Dictionary hinzufügen, um zu sehen, wie sich die Ergebnisse ändern!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Vektor-Embedding"
    ]
  },
  {
    "objectID": "vektor_embedding.html#vom-menschen-zum-wort-vektoren-in-sprachmodellen",
    "href": "vektor_embedding.html#vom-menschen-zum-wort-vektoren-in-sprachmodellen",
    "title": "Vektor-Embedding",
    "section": "Vom Menschen zum Wort: Vektoren in Sprachmodellen",
    "text": "Vom Menschen zum Wort: Vektoren in Sprachmodellen\nDas Grundprinzip ist nun hoffentlich klar: Alles Mögliche ist mithilfe von vielen Eigenschaften als Vektor darstellbar und damit mathematisch vergleichbar. Ein Large Language Model (LLM) macht im Prinzip genau das – nur wendet es dieses Konzept auf Sprache an.\nStatt ganzer Wörter zerlegt ein LLM Text in kleinere Einheiten, sogenannte Tokens (diese sind so gewählt, dass sie einmalig sind). Ein Token kann ein ganzes Wort, ein Wortteil (wie “-ung”) oder ein Satzzeichen sein. Jedes dieser Tokens wird in einen Vektor umgewandelt.\nDie wahre Magie moderner LLMs (wie sie seit den Transformer-Modellen existieren) liegt darin, Kontext zu verstehen. Betrachten wir zwei Sätze:\n\n“A bat is flying around.” (Eine Fledermaus fliegt herum.)\n“I hit the ball with a bat.” (Ich habe den Ball mit einem Schläger geschlagen.)\n\nÄltere Systeme, wie die Online-Übersetzer der 2010er Jahre, hätten für “bat” in beiden Fällen denselben Vektor verwendet und oft falsche Übersetzungen geliefert. Ein modernes LLM hingegen analysiert die umliegenden Wörter (“flying”, “hit the ball”) und erzeugt für “bat” in jedem Satz einen unterschiedlichen Vektor. Der Vektor für “Fledermaus” liegt im Vektorraum dann näher an “Tier” und “fliegen”, während der Vektor für “Schläger” näher an “Sport” und “schlagen” liegt.\nDiese Fähigkeit, Wörter im Kontext zu verstehen, ist der entscheidende Durchbruch, der die heutigen beeindruckenden KI-Leistungen erst möglich macht. Wenn ein LLM dann Text erzeugt, wählt es auf Basis des bisherigen Kontexts das Token aus, dessen Vektor mathematisch am besten als Nächstes passt.\nWie diese Wahl im Detail funktioniert, werden wir im nächsten Kapitel genauer betrachten: Deterministisch vs. Probabilistisch.",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Vektor-Embedding"
    ]
  },
  {
    "objectID": "ki_philosophische_Sicht.html",
    "href": "ki_philosophische_Sicht.html",
    "title": "KI aus der Philosophischen Perspektive",
    "section": "",
    "text": "KI, also Künstliche Intelligenz, beinhaltet offensichtlich den Begriff der Intelligenz. Allein dieser Begriff ist nicht einfach zu fassen: Was bedeutet Intelligenz? Fangen wir klassisch enzyklopädisch an: Intelligenz \\(\\rightarrow\\) inter legere (lat.)\nalso “zwischen” “lesen/wählen”, kann also als die Fähigkeit betrachtet werden, zwischen verschiedenen Optionen zu wählen. Intelligenz ist meistens als Problemlösekompetenz beschrieben.\nAber zurück zum Thema: Es geht darum, inwiefern eine Maschine intelligent sein kann. Hier gibt es viele Debatten – ich hatte schon mindestens drei Seminare zu Thematiken in diese Richtung: * Machine Consciousness (Kurzgesagt die Frage, nach dem Bewusstsein von Menschen und Maschinen) * Determinismus vs. freier Wille (Viele sagen, ein Computer ist nicht “Intelligent”, da er niemals frei entscheiden kann) * strong vs. weak AI (auch wenn man einen Computer als “intelligent” betrachten mag, so doch niemals als menschlich intelligent)\n\n\n\n\n\n\nTipEin paar weiterführende Links\n\n\n\n\n\n\nEin geniales Beispiel der Bewusstseinsfrage spielt sich in “Star Trek: The Next Generation” Staffel 2 Episode 9 ab. (Snodgrass 1989)\n\n\nHintergrundwissen\n\nDazu vielleicht noch ein wenig Hintergrundwissen (falls TNG nicht bekannt): Die Serie spielt im 24. Jahrhundert. Data ist ein Androide, und der Herr in Blau im Clip hat zum Ziel, Data auseinanderzubauen, um weitere wie ihn zu erschaffen. Dies will Data nicht. Es geht in diesem “Gerichtsprozess” darum, ob Data einen eigenen Willen äußern kann oder ob er sich dem Auseinanderbauen unterziehen muss, da er nichts anderes als ein Besitz der Sternenflotte ist.\n\nIm Grunde könnte man hier auch Filme wie Terminator o.ä. verlinken. Die Idee der “Superintelligenz” entspringt der Vorstellung, dass eine KI, sobald sie ein Bewusstsein erlangt, von nichts mehr aufgehalten werden kann.\nDie Unterscheidung zwischen starker und schwacher KI (Strong vs. Weak AI) ist zentral in der KI-Philosophie und geht ebenfalls auf John Searle zurück.\n\nEine schwache KI (Weak AI, auch als narrow AI oder schwache KI bezeichnet) ist darauf ausgelegt, eine spezifische Aufgabe zu erfüllen. Sie simuliert intelligentes Verhalten, ohne jedoch ein echtes Bewusstsein oder Verständnis zu besitzen. Alle heute existierenden KI-Systeme fallen in diese Kategorie.\n\n\nBeispiel für schwache KI\n\nBeispiele hierfür sind KIs, die im Schach, Go oder Starcraft Weltmeister besiegt haben, aber auch Sprachassistenten wie Siri oder eben große Sprachmodelle wie ChatGPT. Sie sind extrem leistungsfähig in ihrem definierten Bereich (z.B. Sprache), aber sie “verstehen” nicht im menschlichen Sinne.\n\nEine starke KI (Strong AI) wäre eine Maschine, die nicht nur Intelligenz simuliert, sondern tatsächlich ein eigenes Bewusstsein, Verstand und Verständnis besitzt – vergleichbar mit einem Menschen. Eine solche KI würde nicht nur Aufgaben lösen, sondern auch subjektive Erfahrungen machen. Dieses Konzept ist rein hypothetisch und wird oft auch als Künstliche Allgemeine Intelligenz (AGI) bezeichnet.\n\n\nBeispiel für starke KI\n\nMan könnte jetzt sagen, aber ChatGPT und Co. sind das doch. NEIN, sind sie nicht. LLMs sind extrem fortgeschrittene schwache KIs. Sie können zwar eine beeindruckende Vielfalt an Aufgaben bewältigen, basieren aber auf Mustererkennung in Daten und besitzen kein Bewusstsein oder echte Intentionalität.\nEine echte starke KI, die sich ihrer selbst bewusst ist, existiert bisher nur in der Science-Fiction (wie z.B. Data aus Star Trek). Die Entwicklung einer solchen KI würde das Konzept der Superintelligenz aufwerfen – eine Intelligenz, die die menschliche in allen Aspekten weit übertrifft.",
    "crumbs": [
      "Grundlagen",
      "Ist das wirklich KI?"
    ]
  },
  {
    "objectID": "ki_philosophische_Sicht.html#grundlage",
    "href": "ki_philosophische_Sicht.html#grundlage",
    "title": "KI aus der Philosophischen Perspektive",
    "section": "",
    "text": "KI, also Künstliche Intelligenz, beinhaltet offensichtlich den Begriff der Intelligenz. Allein dieser Begriff ist nicht einfach zu fassen: Was bedeutet Intelligenz? Fangen wir klassisch enzyklopädisch an: Intelligenz \\(\\rightarrow\\) inter legere (lat.)\nalso “zwischen” “lesen/wählen”, kann also als die Fähigkeit betrachtet werden, zwischen verschiedenen Optionen zu wählen. Intelligenz ist meistens als Problemlösekompetenz beschrieben.\nAber zurück zum Thema: Es geht darum, inwiefern eine Maschine intelligent sein kann. Hier gibt es viele Debatten – ich hatte schon mindestens drei Seminare zu Thematiken in diese Richtung: * Machine Consciousness (Kurzgesagt die Frage, nach dem Bewusstsein von Menschen und Maschinen) * Determinismus vs. freier Wille (Viele sagen, ein Computer ist nicht “Intelligent”, da er niemals frei entscheiden kann) * strong vs. weak AI (auch wenn man einen Computer als “intelligent” betrachten mag, so doch niemals als menschlich intelligent)\n\n\n\n\n\n\nTipEin paar weiterführende Links\n\n\n\n\n\n\nEin geniales Beispiel der Bewusstseinsfrage spielt sich in “Star Trek: The Next Generation” Staffel 2 Episode 9 ab. (Snodgrass 1989)\n\n\nHintergrundwissen\n\nDazu vielleicht noch ein wenig Hintergrundwissen (falls TNG nicht bekannt): Die Serie spielt im 24. Jahrhundert. Data ist ein Androide, und der Herr in Blau im Clip hat zum Ziel, Data auseinanderzubauen, um weitere wie ihn zu erschaffen. Dies will Data nicht. Es geht in diesem “Gerichtsprozess” darum, ob Data einen eigenen Willen äußern kann oder ob er sich dem Auseinanderbauen unterziehen muss, da er nichts anderes als ein Besitz der Sternenflotte ist.\n\nIm Grunde könnte man hier auch Filme wie Terminator o.ä. verlinken. Die Idee der “Superintelligenz” entspringt der Vorstellung, dass eine KI, sobald sie ein Bewusstsein erlangt, von nichts mehr aufgehalten werden kann.\nDie Unterscheidung zwischen starker und schwacher KI (Strong vs. Weak AI) ist zentral in der KI-Philosophie und geht ebenfalls auf John Searle zurück.\n\nEine schwache KI (Weak AI, auch als narrow AI oder schwache KI bezeichnet) ist darauf ausgelegt, eine spezifische Aufgabe zu erfüllen. Sie simuliert intelligentes Verhalten, ohne jedoch ein echtes Bewusstsein oder Verständnis zu besitzen. Alle heute existierenden KI-Systeme fallen in diese Kategorie.\n\n\nBeispiel für schwache KI\n\nBeispiele hierfür sind KIs, die im Schach, Go oder Starcraft Weltmeister besiegt haben, aber auch Sprachassistenten wie Siri oder eben große Sprachmodelle wie ChatGPT. Sie sind extrem leistungsfähig in ihrem definierten Bereich (z.B. Sprache), aber sie “verstehen” nicht im menschlichen Sinne.\n\nEine starke KI (Strong AI) wäre eine Maschine, die nicht nur Intelligenz simuliert, sondern tatsächlich ein eigenes Bewusstsein, Verstand und Verständnis besitzt – vergleichbar mit einem Menschen. Eine solche KI würde nicht nur Aufgaben lösen, sondern auch subjektive Erfahrungen machen. Dieses Konzept ist rein hypothetisch und wird oft auch als Künstliche Allgemeine Intelligenz (AGI) bezeichnet.\n\n\nBeispiel für starke KI\n\nMan könnte jetzt sagen, aber ChatGPT und Co. sind das doch. NEIN, sind sie nicht. LLMs sind extrem fortgeschrittene schwache KIs. Sie können zwar eine beeindruckende Vielfalt an Aufgaben bewältigen, basieren aber auf Mustererkennung in Daten und besitzen kein Bewusstsein oder echte Intentionalität.\nEine echte starke KI, die sich ihrer selbst bewusst ist, existiert bisher nur in der Science-Fiction (wie z.B. Data aus Star Trek). Die Entwicklung einer solchen KI würde das Konzept der Superintelligenz aufwerfen – eine Intelligenz, die die menschliche in allen Aspekten weit übertrifft.",
    "crumbs": [
      "Grundlagen",
      "Ist das wirklich KI?"
    ]
  },
  {
    "objectID": "ki_philosophische_Sicht.html#das-chinese-room-experiment",
    "href": "ki_philosophische_Sicht.html#das-chinese-room-experiment",
    "title": "KI aus der Philosophischen Perspektive",
    "section": "Das Chinese-Room-Experiment",
    "text": "Das Chinese-Room-Experiment\nDas Gedankenexperiment des “Chinesischen Zimmers” wurde 1980 vom Philosophen John Searle vorgestellt (Searle 1980). Es ist das zentrale Argument gegen die Möglichkeit von starker KI und fragt, ob eine Maschine wirklich “verstehen” kann oder nur so tut, als ob.\nStell dir Folgendes vor:\n\nEine Person, die kein Wort Chinesisch spricht, sitzt allein in einem geschlossenen Raum.\nDurch einen Schlitz werden ihr Zettel mit chinesischen Schriftzeichen (Fragen) gereicht.\nIm Raum befindet sich ein riesiges Regelbuch auf Deutsch. In diesem Buch steht genau, welche chinesischen Schriftzeichen (Antworten) sie als Reaktion auf die hereingereichten Zeichen (Fragen) nach draußen geben soll.\nDie Person folgt den Anweisungen, sucht die passenden Symbole und gibt sie als Antwort zurück.\n\nFür einen Beobachter von außen, der Chinesisch spricht, sieht es so aus, als würde die Person im Raum die Fragen perfekt verstehen und beantworten.\nSearles Schlussfolgerung: Die Person im Raum hat absolut kein Verständnis für Chinesisch. Sie manipuliert lediglich Symbole basierend auf einem Regelwerk. Searle argumentiert, dass Computer genauso arbeiten. Selbst wenn eine KI perfekte Antworten gibt, bedeutet das nicht, dass sie ein echtes Bewusstsein oder Verständnis hat. Sie simuliert nur Intelligenz.",
    "crumbs": [
      "Grundlagen",
      "Ist das wirklich KI?"
    ]
  },
  {
    "objectID": "ki_philosophische_Sicht.html#turing-test",
    "href": "ki_philosophische_Sicht.html#turing-test",
    "title": "KI aus der Philosophischen Perspektive",
    "section": "Turing-Test",
    "text": "Turing-Test\nDer Turing-Test, vorgeschlagen von Alan Turing im Jahr 1950, ist eines der ältesten und berühmtesten Kriterien, um maschinelle Intelligenz zu bewerten (Turing 1950). Die Idee ist ein “Imitationsspiel”:\n\nEin menschlicher Fragesteller (C) kommuniziert über Textnachrichten mit zwei ihm unbekannten Gesprächspartnern.\nEiner der Gesprächspartner ist ein Mensch (B), der andere eine Maschine (A).\nDer Fragesteller muss herausfinden, wer von beiden die Maschine ist.\n\nWann besteht die Maschine den Test? Wenn der Fragesteller nach einer angemessenen Zeit nicht zuverlässig sagen kann, wer der Mensch und wer die Maschine ist, hat die Maschine den Turing-Test bestanden. Sie gilt dann als intelligent, weil ihr Verhalten von menschlichem Verhalten nicht zu unterscheiden ist.",
    "crumbs": [
      "Grundlagen",
      "Ist das wirklich KI?"
    ]
  },
  {
    "objectID": "ki_philosophische_Sicht.html#moderne-benchmark-tests",
    "href": "ki_philosophische_Sicht.html#moderne-benchmark-tests",
    "title": "KI aus der Philosophischen Perspektive",
    "section": "Moderne Benchmark Tests",
    "text": "Moderne Benchmark Tests\nDer klassische Turing-Test gilt heute in vielen Fällen als überholt, da moderne LLMs ihn oft problemlos bestehen können. Die Fähigkeit, eine menschliche Konversation zu imitieren, ist nicht mehr der alleinige Maßstab für fortgeschrittene KI.\n\n\n\n\n\n\nWarningHinweis!\n\n\n\n\n\nAuch wenn man sich in der wissenschaftlichen Welt sehr einig ist, dass LLMs den Turing-Test bestehen können, heißt das nicht, dass irgendein System ihn offiziell und anerkannt bestanden hat! Das würde nämlich ein großes neues Problem schaffen, weshalb Firmen wie Google Philosophen anstellen, um genau das zu verhindern. Sollte eine Maschine den Turing-Test bestehen, wäre sie laut dieser Theorie intelligent. Das würde aber auch bedeuten, eine Maschine für Aufgaben wie Hausaufgaben zu nutzen, wäre praktisch Sklaverei. Das klingt absurd, aber was trennt das eine vom Anderen? Was macht Kinderarbeit zu Kinderarbeit? Warum ist es schlimm, wenn Menschen ohne Bezahlung arbeiten? Warum sollte es gut sein, einem intelligenten System den freien Willen zu nehmen?\n\n\n\nZurück zum Thema, heutzutage werden daher andere Arten von Zielen versucht zu erreichen:\n\nIMO (Internationale Mathematik-Olympiade): Lange Zeit galt das Lösen von Problemen auf dem Niveau der IMO als eine große Hürde für KIs. Diese Aufgaben erfordern nicht nur Rechenleistung, sondern auch Kreativität und tiefes logisches Verständnis. Inzwischen gibt es KI-Systeme, die auch hier erstaunliche Leistungen erzielen und Goldmedaillen-Niveau erreichen.\nMMLU (Massive Multitask Language Understanding): Dies ist einer der aktuell gängigsten Benchmarks. Anstatt nur Konversation zu testen, prüft MMLU das Wissen und die Problemlösefähigkeiten einer KI in 57 verschiedenen Fachgebieten – darunter Mathematik, US-Geschichte, Jura, Informatik und mehr. Eine hohe Punktzahl im MMLU-Benchmark zeigt, dass ein Modell über ein breites und tiefes “Wissen” verfügt, das weit über das Führen eines einfachen Gesprächs hinausgeht.\n\n\n\n\nEine Übersicht über Fähigkeit von LLM Anfang 2025\n\n\nHierbei ist wichtig zu beachten, die Modelle die ein wenig aktueller sind, als die unten aufgelisteten haben alle beim IMO Test inzwischen auch 100% erreicht.",
    "crumbs": [
      "Grundlagen",
      "Ist das wirklich KI?"
    ]
  },
  {
    "objectID": "ki_philosophische_Sicht.html#zusammenfassung",
    "href": "ki_philosophische_Sicht.html#zusammenfassung",
    "title": "KI aus der Philosophischen Perspektive",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\nAus philosophischer Perspektive wird KI im Allgemeinen nicht als “intelligent” oder “bei Bewusstsein” bezeichnet. Die meisten folgen hier der technischen Sicht und gehen davon aus, dass ein Programm, egal wie komplex es ist, kein Bewusstsein als solches entwickeln kann.\nEs ist jedoch auch strittig, was überhaupt ein Bewusstsein ausmacht (siehe den Star-Trek-Clip). Vor 50-100 Jahren ging man offiziell davon aus, dass Tiere keinen Schmerz fühlen können und deswegen bei Operationen nicht betäubt werden müssen – sie hätten ja kein Bewusstsein. Dass sie mindestens irgendeine Form von Verstand haben, ist inzwischen klar. Aber ab wann ein Verstand oder Bewusstsein schützenswert ist, ist weiterhin sehr umstritten (siehe die Behandlung von Affen). Daher wird es wohl selbst dann, falls es unstrittig bewusste KI geben sollte, nicht dazu kommen, dass deren Nutzung verboten wird. Und hoffentlich auch nicht dazu, dass sie uns als Superintelligenz direkt ausrottet :P",
    "crumbs": [
      "Grundlagen",
      "Ist das wirklich KI?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alles Gute zum Geburtstag!",
    "section": "",
    "text": "Ich wünsche dir alles Gute zu deinem 59. Geburtstag!\n\n\nIch habe dir hoffentlich bereits alles Gute zum beginnenden 60ten Lebensjahr gewünscht, aber hier erneut alles herzlich Gute zum Geburtstag.\nIch dachte mir, einer Person, die sich alles holen kann, was sie will, kann man am ehesten Zeit schenken. Also dachte ich mir, worüber haben wir schon häufiger geredet, womit wolltest du dich beschäftigen, hast aber vermutlich noch keine Zeit dafür gefunden?\nRichtig: AI, ach nein, KI? Oder doch GPT oder LLM? Was ist denn da jetzt eigentlich der richtige Begriff? Darum, was das alles eigentlich ist, was es bedeutet, wie man es definiert und auch welche moralisch-ethischen Probleme es mit sich bringt – darum soll es auf dieser kleinen Webseite für dich gehen.\n\n\n\n\n\n\nImportantEin paar Hinweise zur Bedienung\n\n\n\n\n\nInhaltsblöcke\n\nCallout-Blöcke: Blöcke wie diesen hier werde ich nutzen, um wichtige Informationen, Tipps oder Warnungen für dich hervorzuheben.\n\nAusklappbare Blöcke: Manchmal werde ich Zusatzinformationen in solchen Blöcken verstecken, um den Text für dich übersichtlich zu halten:\n\n\n\nKlicke hier, um den versteckten Inhalt zu sehen.\n\nDiese Blöcke nutze ich, um Inhalte zu verschachteln, die du nicht sofort lesen musst.\n\n\nSymbole in der Navigationsleiste (oben rechts)\n\nGitHub-Symbol (): Führt dich zum Quellcode dieser Webseite, falls du neugierig bist, wie ich sie für dich gebaut habe.\n\nDesign-Umschalter (Sonne/Mond): Mit diesem Schalter kannst du zwischen dem hellen und dunklen Design wechseln, ganz wie es dir gefällt.\nLupe (): Die Lupe ist die klassische Suchfunktion, um die gesamte Webseite nach Begriffen zu durchsuchen.\nLesemodus-Umschalter: Blendet die linke und rechte Seitenleiste aus, um dir ein ablenkungsfreies Leseerlebnis zu ermöglichen.\n\nBesonderheit auf der “Vektor-Embedding”-Seite\n\nDort habe ich ein interaktives Python-Beispiel für dich vorbereitet. Oben auf der Seite erscheint eventuell eine Eingabeaufforderung für einen API-Schlüssel. Diese kannst du komplett ignorieren. Sie gehört zu einer Zusatzfunktion, die für das Beispiel nicht benötigt wird. Du kannst die Eingabefelder über das kleine Zahnrad-Symbol () daneben einfach ausblenden.\n\n\n\n\nVon jetzt an also gut Klick und Lern!",
    "crumbs": [
      "Begrüßung",
      "Willkommen & Gratulation"
    ]
  },
  {
    "objectID": "chancen_risiken.html",
    "href": "chancen_risiken.html",
    "title": "Chancen & Risiken",
    "section": "",
    "text": "Bevor wir zum Ende kommen, ist es bei einem Thema wie diesem unvermeidbar, auch über die Chancen und Gefahren von KI und LLMs zu sprechen.",
    "crumbs": [
      "Vorteile & Risiken",
      "Chancen & Risiken"
    ]
  },
  {
    "objectID": "chancen_risiken.html#chancen",
    "href": "chancen_risiken.html#chancen",
    "title": "Chancen & Risiken",
    "section": "Chancen",
    "text": "Chancen\n\nAutomatisierung: Es ist ein neuer technologischer Schritt, der sehr vieles automatisieren kann. Dadurch wird die Produktivität der Gesellschaft stark steigen können.\nIndividualbetreuung: Es ist viel leichter, dafür zu sorgen, dass jede Person für ihr spezifisches Problem eine Lösung bekommt. Das funktioniert auch in Schulen und beim Lernen sehr gut.\nWissenschaft: Die Analyse von riesigen Datenmengen wird leichter, ebenso wie Simulationen oder die Erstellung neuer komplexer Systeme (z.B. Proteine).\nWerkzeug: Die KI kann z.B. zum Erstellen von Webseiten wie dieser genutzt werden, sie kann zum Programmieren genutzt werden und für vieles in vielen Bereichen.",
    "crumbs": [
      "Vorteile & Risiken",
      "Chancen & Risiken"
    ]
  },
  {
    "objectID": "chancen_risiken.html#gefahren",
    "href": "chancen_risiken.html#gefahren",
    "title": "Chancen & Risiken",
    "section": "Gefahren",
    "text": "Gefahren\n\nAutomatisierung bedeutet immer auch Arbeitsplatzverlust, mehr Frustration in der Bevölkerung, größeres Krisenpotential\nIndividualbetreuung bedeutet immer auch weniger soziale Interaktion, was zu mehr psychisch Kranken und mehr Vereinsamung führen kann.\nBias: KI lernt aus Datensätzen, wenn die diskriminierend sind, ist die KI es auch\nFehlinformationen: Ähnlich wie bei Taschenrechnern & Co. wird dem System oft mehr geglaubt als der eigenen Intuition. Das heißt, Fehler werden seltener bemerkt.\nManipulation: Konzerne und Länder können KI auf bestimmte Trigger-Worte bestimmt reagieren lassen Beispiel Qwens Einschränkungen (bis unten lesen!)\nFalsches Selbstbewusstsein: LLMs sind darauf trainiert, freundlich und nett zu sein. Dazu gehört auch, einer Person, die für etwas eigentlich nicht bereit ist, Mut zuzusprechen. Das heißt, wenn ich eine KI frage: “Bin ich jetzt gut genug auf die Prüfung vorbereitet?”, wird die KI mir nicht die Wahrheit als Antwort geben, sondern ein: “Du schaffst das!” Hierdurch wird falsche Sicherheit erzeugt.\nFehlende Motivation: Ähnlich wie beim Internet ist es nun noch einfacher, spezifisches Wissen zu lernen. Jeder kann alles lernen, und das relativ schnell. Das sorgt leider auch dafür, dass ein großer Teil der Gesellschaft kein Interesse mehr daran hat, sich damit auseinanderzusetzen. Beispiel Schule: Viele SchülerInnen lösen ihre Hausaufgaben nur noch mit KI. Kann man es ihnen verübeln? Nein. Ist es gut für ihre Bildung? Nein.\nDatenschutz: Der Einsatz von KI wird auf kurz oder lang dafür sorgen, dass die Privatsphäre des Individuums immer weiter eingeschränkt wird. (Wo es früher noch möglich war, manchmal ein bisschen “dumme Sachen” anzustellen, wird das irgendwann nicht mehr möglich sein, weil alles mit Kameras ausgestattet ist, die Bewegungen automatisch wahrnehmen und melden.) Das letzte war ein wenig sehr dystopisch aber betrachten wir alleine die automatische Kennzeichenerfassung von Kennzeichen an vielen Ampeln heute.",
    "crumbs": [
      "Vorteile & Risiken",
      "Chancen & Risiken"
    ]
  },
  {
    "objectID": "chancen_risiken.html#zusammenfassung",
    "href": "chancen_risiken.html#zusammenfassung",
    "title": "Chancen & Risiken",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\nKlar, KI ist eine geniale Erfindung. ABER, wie viele auf der Konferenz, an der ich teilnehmen durfte, feststellten: KI ist besonders dann nützlich, wenn man zu einem Thema bereits etwas weiß. Sich mit KI fortzubilden, geht unglaublich schnell und gut. Aber etwas Unbekanntes zu lernen (oder als unerfahrene Person so etwas zu nutzen), ist nicht zu empfehlen. Dabei ging es noch gar nicht um die Thematik der möglichen Terrorhilfe \\(\\dots\\) Wie schlimm wird Terror, wenn alle Terroristen in der Lage sind, innerhalb von ein paar Stunden herauszufinden, wie man möglichst effizient möglichst viele Zivilisten tötet? (Prima von einer lokalen KI bestimmt \\(\\dots\\)).\nNach diesem hoffentlich nicht zu besorgniserregenden Ende geht es weiter beim Abschied.",
    "crumbs": [
      "Vorteile & Risiken",
      "Chancen & Risiken"
    ]
  },
  {
    "objectID": "abschied.html",
    "href": "abschied.html",
    "title": "Danke",
    "section": "",
    "text": "Ich wollte zum Abschluss nur noch mal Danke sagen.\nDanke dafür, dass du für mich da bist, dich für mich interessierst und mich beglückwünschst, wenn mir etwas Gutes passiert. Es freut mich sehr, dass wir uns so gut verstehen. Ich gehe zwar davon aus, dass wir uns auch mal in die Haare kriegen werden, aber bisher ist das nie geschehen und das freut mich.\nIch freue mich auf viele weitere Jahre als Teil der Familie.\nHab dich lieb und zuletzt, lass die 60er nicht die letzten Jahre sein, zumindest würde ich dich gerne als Großvater erleben, also pass auf dich auf!\nUnd habe ein schönes Jahr, bevor es heißt 60 und du ganz groß feiern musst :P\nBzw. Ich habe dich in der Begrüßung schon versucht ein wenig für das Thema zu erwärmen, das 60. Lebensjahr hat bereits begonnen, man feiert ja immer nur die abgeschlossenen, also falls du die Zahl respektierst/fürchtest, tröste dich, du musst dich nicht mehr fürchten, es ist bereits so weit.\nCheers!!!! Auf dass ich mindestens einen auf dich getrunken habe auf deiner Party!\n\n\n\n\n\n\nTipeine Kleine Kurzzusammenfassung\n\n\n\n\n\nMir ist beim drüber lesen aufgefallen, dass die wirklich wichtigen Links ein wenig untergehen.\nAlso wenn du das Ganze nochmal aus Profi- Sicht, oder besser visualisiert sehen willst:\nDie wichtigsten Links\n\nÜbersicht über die Funktion\n\nTatsächliche Übersicht + Erklärungen\n\nTolle Videos dazu (Playlist)\n\nDas Paper was alles ins Rollen gebracht hat\nMeine Präsentation zu Maschinellem Lernen\nMeine Präsentation zu LLM",
    "crumbs": [
      "Verabschiedung"
    ]
  },
  {
    "objectID": "deterministisch_vs_probabilistisch.html",
    "href": "deterministisch_vs_probabilistisch.html",
    "title": "Deterministisch vs. Probabilistisch",
    "section": "",
    "text": "Ich weiß nicht, ob beide Begriffe allgemein geläufig sind. Deterministisch heißt, dass ein System bei der gleichen Eingabe immer das exakt gleiche Ergebnis liefert, da sein Verhalten vollständig vorherbestimmt ist.\nProbabilistisch heißt, dass ein System bei der gleichen Eingabe unterschiedliche Ergebnisse liefern kann, da sein Verhalten auf Wahrscheinlichkeiten und einem gewissen Grad an Zufall basiert.",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Deterministisch vs. Probabilistisch"
    ]
  },
  {
    "objectID": "deterministisch_vs_probabilistisch.html#begriffsklärung",
    "href": "deterministisch_vs_probabilistisch.html#begriffsklärung",
    "title": "Deterministisch vs. Probabilistisch",
    "section": "",
    "text": "Ich weiß nicht, ob beide Begriffe allgemein geläufig sind. Deterministisch heißt, dass ein System bei der gleichen Eingabe immer das exakt gleiche Ergebnis liefert, da sein Verhalten vollständig vorherbestimmt ist.\nProbabilistisch heißt, dass ein System bei der gleichen Eingabe unterschiedliche Ergebnisse liefern kann, da sein Verhalten auf Wahrscheinlichkeiten und einem gewissen Grad an Zufall basiert.",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Deterministisch vs. Probabilistisch"
    ]
  },
  {
    "objectID": "deterministisch_vs_probabilistisch.html#kontextualisierung",
    "href": "deterministisch_vs_probabilistisch.html#kontextualisierung",
    "title": "Deterministisch vs. Probabilistisch",
    "section": "Kontextualisierung",
    "text": "Kontextualisierung\nOkay, zwei weitere Begriffe, aber was haben diese mit LLMs zu tun? Eine Menge! Wie dir vermutlich schon aufgefallen ist, gibt ein LLM beinahe niemals die gleiche Antwort. Also sind LLMs dann ja probabilistisch, oder? Aber alles, was wir uns dazu angeguckt haben – also dass immer nur das nächste Token (Wort) bestimmt wird – passt nicht ganz zu dieser Ansicht.\nTatsächlich sind LLMs im Allgemeinen probabilistisch, und das mit Absicht. Das ist eine der Genialitäten an ihnen: Dadurch, dass ein LLM nicht stur das wahrscheinlichste nächste Wort wählt, ist es in der Lage, kreative und neue Texte zu schreiben, anstatt nur im eigenen “Bias” zu verharren.\n\n\nFalls Bias nicht bekannt sind\n\nBias (aus dem Englischen) bedeutet im Deutschen so etwas wie Voreingenommenheit oder Vorurteil. In der Wahrscheinlichkeitstheorie, aber auch in der Forschung generell, sind sie ein großes Problem bei der Suche nach objektivem Wissen. Auch LLMs bzw. KI können einen Bias entwickeln oder aufweisen. Dies lässt sich relativ leicht zeigen, indem man einer beliebigen Bild-KI die Aufgabe gibt, eine “Gruppe von Menschen” zu generieren. Meistens werden fast alle dargestellten Menschen mitteleuropäisch und weiß sein. Es gibt auch schlimmere Bias. Man kann sich das leicht vorstellen: Das LLM übernimmt, womit es trainiert wird. Wird es mit rassistischem Material trainiert, wird es rassistisch sein. Ein bekanntes Beispiel ist “Grok”. Berichten zufolge hat Elon Musk versucht, das LLM so auszurichten, dass es bestimmte politische Ansichten fördert. Da es aber – wie alle großen Modelle – auf einem riesigen, allgemeinen Datensatz vortrainiert wurde, hat das Modell oft “gelernt”, dass solche extremen Aussagen nicht der Norm entsprechen.",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Deterministisch vs. Probabilistisch"
    ]
  },
  {
    "objectID": "deterministisch_vs_probabilistisch.html#visualisierung",
    "href": "deterministisch_vs_probabilistisch.html#visualisierung",
    "title": "Deterministisch vs. Probabilistisch",
    "section": "Visualisierung",
    "text": "Visualisierung\nOkay, es gibt hier gleich eine super Webseite (Wang 2023), auf der man viel sehen kann, was sehr hilfreich ist. Aber vorher will ich sie ein wenig erklären. Auf der Webseite wird ein Transformer-Modell mitsamt der unterschiedlichen Teilschritte gezeigt. Oben kann ein Satz eingegeben werden, und im Hauptteil wird dann visualisiert, wie die möglichen nächsten Wörter bestimmt werden.\nDas ist die Webseite von Poloclub! Ich kann diese Seite für ihre Anschaulichkeit gar nicht genug loben!\nHierbei geht es vor allem um drei Parameter: Temperature, Top-k und Top-p.\n\nTop-k bestimmt die Anzahl der wahrscheinlichsten Wörter, die für die nächste Auswahl überhaupt in Betracht gezogen werden. Wenn man an dem Regler spielt, sieht man, dass mehr Wörter eine Wahrscheinlichkeit zugewiesen bekommen (die anderen haben die Wahrscheinlichkeit 0).\nTop-p funktioniert ähnlich, wählt aber nicht eine feste Anzahl, sondern so viele Wörter, bis deren summierte Wahrscheinlichkeit einen bestimmten Schwellenwert (p) erreicht.\nTemperatur bestimmt die Gewichtung der Wörter,dadurch wird “Kreativität” bzw. Zufälligkeit der Auswahl verändert. Eine hohe Temperatur macht die Wahrscheinlichkeiten der möglichen Wörter ähnlicher, was zu überraschenderen Ergebnissen führt. Eine niedrige Temperatur verstärkt die wahrscheinlichsten Optionen und macht die Antwort vorhersagbarer.\n\n\n\nEine Webseite die ich gerade noch gefunden habe, auf der die Begriffe nett erklärt werden\n\nVellum.ai\nIch hoffe, falls meine Erklärung nicht gut genug war, ist es zumindest die auf der Webseite.\nUnd eine GANZ wichtige Sache ist mithilfe dieser Seite zu sehen, stellt man einen dieser zwei Werte (Top-k und Top-p sind Entweder oder) maximal restriktiv ein, so gibt es nur eine Möglichkeit.\nDas heißt aber nichts anderes, als diese zwei(drei) Parameter sind das Einzige, was ein LLM probabilistisch macht, anstatt deterministisch!\n\nEin kleiner Hinweis noch: Es gibt nicht nur diese eine Webseite, die das visualisiert, aber in der schnelllebigen Zeit der KI ändern sich Webseiten leider täglich. Zu Beginn meiner ChatGPT- und Copilot-Nutzung konnte man in beiden Modellen angeben, ob man die Antwort lieber “präzise” oder “kreativ” haben wollte. Das ist nichts anderes als ein voreingestellter Wert für Temperatur, Top-k oder Top-p.\nEs gibt auch immer noch einige Seiten, auf denen man ausprobieren kann, was z.B. passiert, wenn man die Temperatur auf 1 dreht (die LLM können dann meist ihre Antwort nicht mehr stoppen, sowie Worte bilden etc.)\n(Ich finde leider auf Anhieb kein Open Source online nutzbare Variante -Ich kann dir das die Tage mal mit meinem Uni Zugang zeigen-)\nAnsonsten könnte man evtl. über Huggingface.co eine KI herunterladen und es dann lokal ausprobieren.",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Deterministisch vs. Probabilistisch"
    ]
  },
  {
    "objectID": "ki_allgemein.html",
    "href": "ki_allgemein.html",
    "title": "KI im Allgemeinen",
    "section": "",
    "text": "Mit KI und LLMs wird seit etwa dem Zweiten Weltkrieg geforscht, auch (Turing 1950) ist daran beteiligt. Über die Jahrzehnte hat sich einiges getan, aber wie den meisten aufgefallen ist, ist “plötzlich KI überall”. Das lässt sich gut in dem folgenden Bild erkennen:\n (Wang et al. 2024)\nWie im Bild auch zu sehen ist, geht ab der Erfindung des Transformer-Modells alles sehr schnell. Das relevante Paper wurde von (Vaswani et al. 2017) veröffentlicht.\n\n\nWarum passen die Jahreszahlen 2015 und 2017 nicht zusammen? Klicke hier!\n\nDas Paper von (Vaswani et al. 2017) ist zwar nicht der Ursprung des Attention-Algorithmus, aber es nutzt diese Idee von 2014, um das Transformer-Modell zu entwickeln. Als kleiner Nebenhinweis “GPT” ist vermutlich ein Begriff, steht für “Generative Pre-trained Transformer”.\nDaran kann man erkennen, wie wichtig dieses Paper ist: Die relevanteste KI unserer Zeit trägt es im Namen.",
    "crumbs": [
      "Grundlagen",
      "KI im Allgemeinen"
    ]
  },
  {
    "objectID": "ki_allgemein.html#historie",
    "href": "ki_allgemein.html#historie",
    "title": "KI im Allgemeinen",
    "section": "",
    "text": "Mit KI und LLMs wird seit etwa dem Zweiten Weltkrieg geforscht, auch (Turing 1950) ist daran beteiligt. Über die Jahrzehnte hat sich einiges getan, aber wie den meisten aufgefallen ist, ist “plötzlich KI überall”. Das lässt sich gut in dem folgenden Bild erkennen:\n (Wang et al. 2024)\nWie im Bild auch zu sehen ist, geht ab der Erfindung des Transformer-Modells alles sehr schnell. Das relevante Paper wurde von (Vaswani et al. 2017) veröffentlicht.\n\n\nWarum passen die Jahreszahlen 2015 und 2017 nicht zusammen? Klicke hier!\n\nDas Paper von (Vaswani et al. 2017) ist zwar nicht der Ursprung des Attention-Algorithmus, aber es nutzt diese Idee von 2014, um das Transformer-Modell zu entwickeln. Als kleiner Nebenhinweis “GPT” ist vermutlich ein Begriff, steht für “Generative Pre-trained Transformer”.\nDaran kann man erkennen, wie wichtig dieses Paper ist: Die relevanteste KI unserer Zeit trägt es im Namen.",
    "crumbs": [
      "Grundlagen",
      "KI im Allgemeinen"
    ]
  },
  {
    "objectID": "ki_allgemein.html#einführung",
    "href": "ki_allgemein.html#einführung",
    "title": "KI im Allgemeinen",
    "section": "Einführung",
    "text": "Einführung\nKünstliche Intelligenz (KI) beschreibt Maschinen oder Software, die so gestaltet wurde, dass sie Aufgaben übernimmt, die üblicherweise menschliche Intelligenz erfordern. Dazu gehören Wahrnehmung, Sprachverstehen, Planen, Lernen und Problemlösen.\n\nAn AI system is a machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.\n\n— (“OECD AI Principles” 2019)",
    "crumbs": [
      "Grundlagen",
      "KI im Allgemeinen"
    ]
  },
  {
    "objectID": "ki_allgemein.html#hauptansätze-der-ki",
    "href": "ki_allgemein.html#hauptansätze-der-ki",
    "title": "KI im Allgemeinen",
    "section": "Hauptansätze der KI",
    "text": "Hauptansätze der KI\nKünstliche Intelligenz ist kein monolithisches Feld, sondern unterteilt sich in verschiedene Ansätze und Techniken. Die wichtigsten davon werden hier vorgestellt.\n\nSymbolische KIMaschinelles Lernen (ML)Deep LearningNeuronale Netze\n\n\nSymbolische KI (auch „Good Old-Fashioned AI“) basiert auf Regeln, Logik und Wissensrepräsentation. Entwickler definieren explizit das Wissen und die Regeln, nach denen das System Entscheidungen trifft.\n\nFunktionsweise: Arbeitet mit Symbolen und logischen “Wenn-Dann”-Regeln.\nVorteil: Die Entscheidungen sind transparent und nachvollziehbar.\nNachteil: Sehr starr und schlecht skalierbar für komplexe, unstrukturierte Probleme wie Bild- oder Spracherkennung.\n\nFür weiteres: (“Symbolic Artificial Intelligence — Wikipedia” 2025)\n\n\nBeim maschinellen Lernen (ML) werden die Regeln nicht fest programmiert. Stattdessen lernt ein Algorithmus aus Daten, Muster zu erkennen und Vorhersagen zu treffen. Es ist der am weitesten verbreitete Ansatz in der modernen KI.\nMan unterscheidet hauptsächlich drei Arten:\n\nÜberwachtes Lernen (Supervised Learning): Das Modell lernt von Daten, die bereits mit der richtigen Antwort (“Label”) versehen sind (z.B. Bilder von Katzen mit dem Label “Katze”).\nUnüberwachtes Lernen (Unsupervised Learning): Das Modell findet eigenständig Muster und Strukturen in ungelabelten Daten (z.B. Kundensegmentierung).\nVerstärkendes Lernen (Reinforcement Learning): Ein “Agent” lernt durch Versuch und Irrtum, indem er für gute Aktionen belohnt und für schlechte bestraft wird (z.B. beim Training einer KI für ein Brettspiel).\n\n\n\n\n\n\n\nNoteEin kleiner Grundriss (Ein Vortrag, den ich dazu mal gehalten habe)\n\n\n\n\n\n\n\nIhr Browser unterstützt keine eingebetteten PDFs. Sie können es stattdessen hier herunterladen.\n\n\nBei Fragen zu dem Vortrag, frag mich gerne :D\n\n\n\nFür weiteres: (Kochenderfer et al. 2020; “Machine Learning — Wikipedia” 2025)\n\n\nDeep Learning ist ein Teilgebiet des maschinellen Lernens, das besonders tiefe (im Sinne von vielen Lagen hintereinander) neuronale Netze verwendet. Diese Tiefe ermöglicht es dem Modell, sehr komplexe und hierarchische Merkmale aus den Daten zu lernen.\n\nMerkmalshierarchie: Die ersten Schichten erkennen einfache Merkmale (z.B. Kanten in einem Bild), während tiefere Schichten diese zu komplexeren Konzepten (z.B. Augen, Gesichter) zusammensetzen.\nAnwendungen: Fast alle modernen Durchbrüche wie Bilderkennung, Sprachübersetzung (Google Translate) und selbstfahrende Autos basieren auf Deep Learning. LLMs sind ebenfalls eine Form des Deep Learning.\n\nFür weiteres: (LeCun, Bengio, and Hinton 2015; “Deep Learning — Wikipedia” 2025)\n\n\nNeuronale Netze sind das Herzstück des Deep Learning. Sie sind von der Struktur des menschlichen Gehirns inspiriert und bestehen aus miteinander verbundenen “Neuronen”, die in Schichten (Layern) angeordnet sind.\n\nAufbau: Jedes Neuron empfängt Signale, verarbeitet sie und gibt ein eigenes Signal an die nächste Schicht weiter. Die Verbindungen zwischen den Neuronen haben “Gewichte”, die im Lernprozess angepasst werden.\nFunktion: Durch die Anpassung dieser Gewichte lernt das Netz, komplexe Muster in den Daten zu erkennen – zum Beispiel die Pixelmuster, die eine Katze auf einem Bild ausmachen.\n\nEin fantastisches Video, das die Grundlagen visuell erklärt, ist von 3Blue1Brown: (Sanderson 2017)",
    "crumbs": [
      "Grundlagen",
      "KI im Allgemeinen"
    ]
  },
  {
    "objectID": "ki_allgemein.html#anwendungsbereiche",
    "href": "ki_allgemein.html#anwendungsbereiche",
    "title": "KI im Allgemeinen",
    "section": "Anwendungsbereiche",
    "text": "Anwendungsbereiche\nNachdem nun klarer ist, was KI ist und welche Ansätze es gibt, hier ein kurzer Blick auf die Anwendungsbereiche. Am bekanntesten sind derzeit sicherlich die Large Language Models (LLMs) wie ChatGPT. Aber KI steckt auch in vielen anderen Technologien, wie zum Beispiel in der Gesichtserkennung auf dem Smartphone oder in Überwachungskameras.",
    "crumbs": [
      "Grundlagen",
      "KI im Allgemeinen"
    ]
  },
  {
    "objectID": "ki_allgemein.html#vertiefungsmöglichkeiten",
    "href": "ki_allgemein.html#vertiefungsmöglichkeiten",
    "title": "KI im Allgemeinen",
    "section": "Vertiefungsmöglichkeiten:",
    "text": "Vertiefungsmöglichkeiten:\nFür alle, die tiefer einsteigen wollen, hier ein paar Empfehlungen. Diese YouTube-Videos werden oft auch von Dozenten an Universitäten empfohlen:\n\nDer Kanal 3Blue1Brown ist generell eine Goldgrube für visuelle Erklärungen zu Mathe und Informatik.\nSpezifische Videos zum Thema:\n\nWas ist ein neuronales Netz?\nWie neuronale Netze lernen\nDer Attention-Mechanismus (Grundlage für Transformer)\n\n\nWeitere Berichte und Prinzipien finden sich bei der OECD und Stanford: (“OECD AI Principles” 2019; “AI Index Report — Stanford HAI” 2025)",
    "crumbs": [
      "Grundlagen",
      "KI im Allgemeinen"
    ]
  },
  {
    "objectID": "konzept_llm.html",
    "href": "konzept_llm.html",
    "title": "LLM – Das Grundkonzept",
    "section": "",
    "text": "Besser als ich es erklären kann, wird es im Zweifel hier von 3blue1brown auf YouTube erklärt, aber ich werde mich mal dran versuchen.",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Grundkonzept LLM"
    ]
  },
  {
    "objectID": "konzept_llm.html#was-ist-ein-llm",
    "href": "konzept_llm.html#was-ist-ein-llm",
    "title": "LLM – Das Grundkonzept",
    "section": "Was ist ein LLM?",
    "text": "Was ist ein LLM?\nEin Large Language Model (LLM), auf Deutsch ein großes Sprachmodell, ist eine Art von künstlicher Intelligenz, die darauf trainiert ist, menschliche Sprache zu verstehen und zu erzeugen. Man kann es sich als ein extrem großes neuronales Netz vorstellen, das mit einer gigantischen Menge an Textdaten (wie Büchern und großen Teilen des Internets) trainiert wurde. Das Hauptziel eines LLM ist es, auf eine Texteingabe (einen “Prompt”) eine sinnvolle und kontextbezogene textliche Antwort zu generieren. Bekannte Beispiele sind die Modelle der GPT-Reihe (die hinter ChatGPT stehen), Googles Gemini oder Metas Llama.",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Grundkonzept LLM"
    ]
  },
  {
    "objectID": "konzept_llm.html#wie-funktioniert-ein-llm",
    "href": "konzept_llm.html#wie-funktioniert-ein-llm",
    "title": "LLM – Das Grundkonzept",
    "section": "Wie funktioniert ein LLM",
    "text": "Wie funktioniert ein LLM\nDie Funktionsweise eines LLM lässt sich vereinfacht in zwei Hauptphasen unterteilen: das Training und die Anwendung (auch Inferenz genannt).\n\nPhase 1: Das Training (Lernphase)\n\nDaten: Das Modell wird mit riesigen Mengen an Texten gefüttert.\nZiel: Die Kernaufgabe während des Trainings ist simpel, aber wirkungsvoll: “Sage das nächste Wort voraus.” Das Modell bekommt einen Satzanfang und versucht, das wahrscheinlichste nächste Wort zu erraten.\nLernprozess: Bei jeder falschen Vorhersage passt es seine internen Parameter an. Durch milliardenfache Wiederholung lernt es so Grammatik, Fakten, Zusammenhänge und sogar Argumentationsstile. Es entwickelt ein statistisches “Verständnis” von Sprache.\n\nPhase 2: Die Anwendung (Inferenz)\n\nEingabe: Du gibst eine Frage oder einen Befehl ein (der “Prompt”).\nGenerierung: Basierend auf deinem Prompt und seinem Training berechnet das LLM das wahrscheinlichste erste Wort der Antwort.\nWort für Wort: Dieses neue Wort wird an die bisherige Konversation angehängt, und der Prozess wiederholt sich: Das Modell berechnet das nächste wahrscheinlichste Wort, dann das übernächste und so weiter, bis eine vollständige Antwort entstanden ist.\n\n\nDie erste Phase ist hierbei wie bei jeder anderen Art des Maschinellen Lernens.\nDie zweite Phase hingegen ist besonders: Hier geht es darum, einen Text weiterzuschreiben. Für das LLM gibt es nämlich keinen “Chat”, sondern nur einen einzigen, fortlaufenden Text. Unsichtbar für den Nutzer wird nach jeder Eingabe und vor jeder Ausgabe ein interner Erklärtext eingefügt, der dem Modell den Kontext gibt.\n\n\n\n\n\n\nTipAuch hierzu musste ich mal einen Vortrag in der Uni halten\n\n\n\n\n\nDer Vortrag bzw. die Folien des Vortrags sind mehr oder weniger das ganze Kapitel in anderer Form (nicht nur dieses Unterkapitel, also evtl. erst im Nachhinein angucken :shrug:).\n\n\nIhr Browser unterstützt keine eingebetteten PDFs. Sie können es stattdessen hier herunterladen.\n\n\n\n\n\nUm die Funktionsweise besser zu verstehen, solltest du dir die nächsten Kapitel durchlesen. Dort wird erklärt, wie ein LLM Wörter überhaupt mathematisch versteht (Vektor-Embedding) und wie es im Detail aufgebaut ist.",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Grundkonzept LLM"
    ]
  },
  {
    "objectID": "konzept_llm.html#weiterführende-quelle",
    "href": "konzept_llm.html#weiterführende-quelle",
    "title": "LLM – Das Grundkonzept",
    "section": "Weiterführende Quelle",
    "text": "Weiterführende Quelle\nEine komprimierte Übersicht über Large Language Models findet man bei (Naveed et al. 2024) Eine sehr gute Seite, die einen kompletten Überblick über Transformer-Modelle inklusive Erklärungen bietet, ist bbycroft.net/llm. Diese Seite ist, ähnlich wie die aus dem Kapitel “Deterministisch vs. Probabilistisch”, sehr zu empfehlen.",
    "crumbs": [
      "Wie LLMs funktionieren",
      "Grundkonzept LLM"
    ]
  }
]